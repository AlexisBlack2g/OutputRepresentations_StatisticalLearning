
#Introduction
In our daily lives, we are inundated with streams of sights, sounds, smells, and tactile sensations. Our experience of this is guided and streamlined by a set of expectations about how the world works. Yet how do we form these expectations? While no single mechanism can account for learning of all the perceptual categories that constrain the flow of sensory information, over the last few decades one mechanism has been implicated as a fundamental contributor: statistical learning. Statistical learning (SL) is, roughly, the capacity to induce structure from statistical patterns that are distributed across streams of sensory input [@saffran_statistical_1996; @maye2002infant; @christiansen2018implicit]. The human capacity for SL has been successfully demonstrated across perceptual domains, (e.g., vision: Kirkham, Slemmer, & Johnson, 2002; audition: Saffran, Johnson, Newport, & Aslin, 1999; touch: Conway & Christiansen, 2005; visuomotor: Hunt & Aslin, 2001), is relatively automatic and robust to sensory interference (Saffran, Newport, Aslin, Tunick, & Barrueco, 1997; Turk-Browne, Scholl, Chun, & Johnson, 2009; cf. Toro, Sinnett, & Soto-Faraco, 2005), and is operable by the time an infant is born [@teinonen2009statistical; @bulf2011visual; @kudo2011line].   
While the power and ubiquity of SL has made it a compelling mechanism for theories of perceptual learning generally [@aslin2017statistical], there is no domain in which it has had more of a theoretical impact than that of language acquisition (see, e.g., Aslin & Newport, 2012; Kuhl, 2004). Indeed, SL has been hypothesized to contribute to the acquisition of nearly every level of linguistic hierarchy: phonological categories [@maye2002infant; @noguchi2018emergence], words [@saffran_statistical_1996; @graf2007meaning], syntactic classes and combinatorial rules [@saffran2003syllables; @thompson2007statistical; @finn2014hurts], and semantic networks [@smithInfantsRapidlyLearn2008; @yurovskyCompetitiveProcessesCrossSituational2013]. Yet from the earliest days of the SL literature, researchers have disagreed about the nature of the computational and perceptual processes that underlie it, asking, for example, whether learners compute the statistical relationships between units [see @saffranInfantStatisticalLearning2018], or if the input is chunked and encoded in a way that has no direct relationship to the underlying statistics, but yields comparable final structures in memory (see Thiessen, 2017).  
In this manuscript, we examine the nature of the output of auditory statistical learning as a means of elucidating the mechanism(s) that underlie it. Over 3 experiments, we find that SL involves more than (or something other than) tracking the statistical relationships between sounds. We further reveal nuanced influences of perception and executive function on the learning process and its outcomes.  In the following paragraphs we outline the original paradigm that forms the basis of our research, and situate this paradigm within the linguistic context it was  intended to address: the problem of word segmentation.   

##Background
The idea that learners can use statistical cues in their environment to induce linguistic categories has a long history (e.g., Harris, 1955; Hayes & Clark, 1970); however, it was a seminal study by Saffran, Aslin, and Newport (1996) that brought the idea to the forefront of theories of language acquisition. In this study, the authors addressed the dilemma of word-segmentation: how do infants learn where word boundaries are when there are no unique, consistent phonetic cues to signal them, either across languages (e.g., Cutler & Carter, 1987), or – even more strikingly – within a single language (e.g., Cole & Jakimik, 1980; Dumay, Content & Frauenfelder, 1999; Klatt, 1980)? The authors propose two hypotheses: (1) sequence transitions within words occur with higher probability than those across word boundaries, and (2) infants can use this information to postulate word boundaries (Saffran et al., 1996).  
To test this second hypothesis, Saffran et al. (1996) presented 8-month-old infants with a brief, continuous stream comprised of four trisyllabic nonce words that repeated in a semi-random order. Importantly, the 12 unique syllables that made up these words were stripped of any informative prosodic or phonetic cues to the word boundaries. To extract the boundaries, the infants had to recruit the differential statistical relationship between syllables within the words as opposed to across them. This statistical relationship was defined as transitional probability (TP), that is, the frequency that two sounds (in this case syllables) co-occur as a proportion of the raw frequency of one of them. Within the stream, four trisyllabic sequences occurred with a TP of 1.0 between syllables; these sequences were called the “words” of the language. Words were interlaced in such a way that between-word syllable transitions maintained a 0.33 TP.  
After familiarization, infants were tested on their knowledge of the underlying TP-defined structure. Two versions of the study tested infants’ preference for words (the high TP trisyllabic sequences that made up the familiarization stream) versus non-word foils (novel trisyllabic combinations from the same set of syllables), and words versus part-word foils (trisyllabic sequences that had been encountered in the familiarization stream, but which included a word boundary). In both cases, infants responded differently to the high-TP and low-TP structures, suggesting that infants might indeed segment words by attending to transitional statistics (Saffran et al., 1996).  
It is worth pausing to consider what is meant by 'word' in this context. Within the acquisition literature, the term word typically refers to a chunk of phonetic material that is maintained in memory and is associated with some constellation of semantic features/contexts. In other words, if an infant has extracted the phonetic chunk /da/, and recognizes that chunk as being associated with a particular context, it is sufficiently word-like to be considered a word. Note that a child's word need not reflect the adult target: it might be reduced (e.g., “da” for dog), match only some (or even none) of the adult target sounds (e.g. “bo” for sun), consist of multiple morphemes (for an adult, e.g., “singing”), or it might even consist of multiple words (e.g., "allgone" treated as a single word). A word's most salient feature is simply that it is a stable (but not necessarily static) acoustic form, recognized as a singular chunk by the infant. Typically, however, this form is paired with some consistent (even if low-level/underspecified) meaning. This is clearly not the case in the word-segmentation SL paradigm: unless there is also training on sound-object pairings post familiarization, there is no obvious semantics for a learner to associate with acoustic strings extracted from continuous speech. Thus, it would seem that the SL literature posits that the outcome of SL is a stable acoustic form, available for association with semantics.  
Several studies have demonstrated the viability of this definition. For example, after exposure to a continuous, TP-defined stream, infants are more likely to learn semantic associations with high-TP than with low-TP units (Graf Estes et al., 2007; Hay, Pelucchi, Graf Estes & Saffran, 2011; Shoaib, Wang, Hay, & Lany, 2018). Infants have also been shown to more readily incorporate high-TP units extracted from a continuous stream into fluent native language speech (Saffran, 2001). Finally, differential TP-strength of strings has been shown to interact with a classic categorization effect in infants. In this paradigm, when infants view a series of visual objects occurring simultaneously with a single, repeating acoustic stimulus, they learn to treat those visual objects as belonging to a single category (e.g., Waxman & Braun, 2005). This process of categorization is facilitated when the acoustic label reflects a high-TP sequence from a previously presented familiarization language, in contrast to a low-TP sequence from that language (in the latter case, no cateogry-learning was evidenced; Erickson, Thiessen & Graf Estes, 2014). These findings all show differential behaviour related to TP-strength. The relationship between these outcomes and the underlying learning process is not as clear as it may seem, however, as each of these findings is consistent with two possible SL processes: either learners extract a particular structure that is then established in memory as an independent chunk (e.g., a word), or learners entrain to the veridical TP-structure, but do not extract independent chunks in the absence of association with some additional cue (e.g., semantics, or a cue that is itself associated with boundaries, such as silence).   

###TP-encoding. 
A straightforward account of SL would posit that learners extract the TP-structure, and this is supported by a number of findings in the literature. For example, one puzzle in SL tasks is that participants rarely perform at ceiling in standard SL paradigms (Siegelman, Bogaerts, & Frost, 2017), despite the fact that the optimal segmentation of the stream is clearly defined (e.g., four trisyllabic words of 1.0 TPs in the original Saffran et al., 1996). If learners set word boundaries around the TP-defined word edges, research suggests that once even a single word has been extracted the others should soon follow (Bortfeld, Morgan, Golinkoff, & Rathbun, 2005; Dahan & Brent, 1999). This does not appear to be the case. Not only are learners rarely aware of or particularly successful at explicitly identifying the underlying structure post-exposure, but even giving learners one of the high-TP words in advance of exposure has been shown to have no facilitatory effect on learning (Finn & Hudson Kam, 2008). And, while there is some evidence that the presence of a familiar word enhances infants’ ability to parse a continuous SL stream (Mersad & Nazzi, 2012), infants are surprisingly sensitive to the consistency of the underlying TP structure, and generally fail when the embedded words are of different syllable lengths (Johnson & Tyler, 2010; Mersad & Nazzi, 2012; cf. Erickson et al., 2014). Thus, while both children and adults appear to treat TP-defined nonce words as viable word candidates (Erickson et al., 2014; Graf Estes et al., 2007; Hay et al., 2011; Saffran, 2001; Shoaib et al., 2018), their failure to fully parse the stream suggests that learning consists of TP-tracking alone.   
Finally, one feature that characterizes the word-forms stored in adult lexicons is knowledge not just of the sequential nature of the embedded sounds, but also the relative positions of (at least some of) those sounds (Allopenna, Magnuson, & Tanenhaus, 1998; Brown & McNeill, 1966; MacKay, 1970; Marslen-Wilson & Zwitserlood, 1989; Swingley, Pinto & Fernald, 1999). For example, the adult representation of the word *dog* consists both of the fact that /d/ is followed by /a/, but also that /d/ is the initial sound in the word – a position that it shares with a large number of other possible words (e.g. *doll*). Work that has looked for position-based encoding under SL conditions has met with largely negative results (Endress & Bonatti, 2007; Endress & Mehler, 2009a; Peña, Bonatti, Nespor, & Mehler, 2002). For instance, in Peña et al. (2002), learners attended to a stream of trisyllabic sequences of the type A~1~XC~1~, in which A~1~ was entirely predictive of C~1~, but syllable X varied. Learners successfully used this non-adjacent dependency to segment the speech stream; however, they failed to generalize the relationship between A and C to novel X combinations. In fact, the longer the familiarization, the more likely participants were to choose low-TP (i.e., non-word) trisyllabic sequences that they had encountered (e.g., C~2~#A~1~X), as opposed to rule-based A~1~XC~1~ combinations with novel middle (X) syllables. When participants were given pre-segmented words in familiarization (i.e., the words were flanked by brief pauses), however, they quickly extracted the necessary generalization, and picked forms that followed the A1XC1 rule, irrespective of adjacent TPs. Endress and Bonatti (2007) and Endress and Mehler (2009b) extended this work to show that learners can induce classes of syllables that belong in edges (the first or last syllable of multisyllabic words), but fail to do so with internal constituents – and, once again, can only do so when the words are bracketed with a prosodic cue (i.e., subliminal pauses between words, or final syllable lengthening). Taken together, these studies paint a picture of SL as a mechanism that involves primarily (or solely) the extraction of TPs between syllables, or adjacent to locally non-adjacent segments (Newport & Aslin, 2004; see Creel, Newport & Aslin, 2004 for parallel results with pure tone stimuli). Thus, it would seem that SL involves tracking of adjacent transitional probabilities, and storing those TPs in some relatively continuous (i.e., non chunk-like) way.     

###Position-encoding.
Despite the findings discussed in the previous section, other studies suggest that emergent representations from SL bear independent, chunk-like features as a result of the SL process itself. Studies of visual perception show that once we perceive a whole, conscious recognition of and memory for the lower-level features within it decreases, both in on-line processing and in short-term memory (e.g., Navon, 1977; Poljac, de-Wit, & Wagemans, 2012). Parallels to this gestalt-like phenomenon have been noted in both the auditory and visual SL literatures. For instance, in a study in which participants were trained on a continuous stream composed of di- and tri-syllabic words, Giroux and Rey (2009) found that participants recognized full words better than partial words (i.e., disyllables extracted from tri-syllable full words) after 10 minutes of exposure, but recognized both at equivalent levels of performance with less exposure. This result is compatible with a kind of chunking or gestalt-like process. Since both partial words and full words are equally good TP sequences, they should be equally recognizable after sufficient training; the fact that memory for partial words suffers in comparison to memory for words suggests that the larger structure supercedes memory for/recognition of the substructures from which it is built.   
Fiser and Aslin (2005) similarly demonstrated in visual SL that learners’ memories for sub-structural components declines in comparison to their memories for the same aspects of images that are not grouped in a single structure. Adult learners were exposed to visual arrays of novel shapes that were grouped into pairs or quadruples. When learners were tested on their discrimination of pairs they had experienced and novel pairs, they only succeeded when the familiar pair had not been embedded in a quadruple-image structure. Those pairs that had been embedded in larger structures were indistinguishable from novel, unfamiliar pairs. More recently, Zhao and Yu (2016) demonstrated that adults’ perception of the number of dots in an array reduces as a function of statistically-defined embedded pairs. This finding is particularly striking given that learners failed to distinguish high-TP combinations from foils at test, suggesting that, while exposure to the stream had not yet induced robust enough categories to withstand an explicit 2-alternative forced-choice (2AFC) test, nascent category-level representations were influencing learners’ perception.   
These studies provide evidence that representations that emerge from SL are characterized by more than simply the sum of their statistics. In addition, despite the aforementioned evidence that words learned via SL do not involve position-based encoding, other findings point to asymmetrical encoding of syllables across different locations during SL. For instance, in both Saffran, et al. (1996) and Saffran, et al. (1999) (and as discussed in Johnson, 2012), participants were better able to reject non-words of the structure ABX than of the structure XBC (where ABC represents the three syllables of a nonce word, and X reflects a randomly chosen syllable that did not occur in those sequences). By itself, this result is rather opaque. Perhaps the coherence of medial and final syllables is more strongly encoded than that of initial and medial syllables, which would then lead to better recognition of ABX as violating this coherence as compared to XBC. Alternatively, learners might have encoded the position of word-final syllables (but not/less-so the position of initial syllables), which would lead to easier detection of the word-final illicit syllable.  Regardless of the cause, however, the finding that encoding is asymmetrical across extracted sequences is unexpected under a simple TP-tracking account of SL.\footnote{Note that if learners are tracking solely forward TPs, we might expect that, given a sequence ABC, the association between B and C is more predictable/lower in information theoretic content than that between A and B, since A fully predicts B, which in turn fully predicts C. This could then lead to differential effects of learning across the three syllable positions. Research has clearly demonstrated, however, that learners track both forward and backwards TPs (Onnis \& Thiessen, 2013; Pelucchi, Hay, \& Saffran, 2009; Perruchet \& Desaulty, 2008; Thiessen, Onnis, Hong \& Lee, 2019). In the case of these two Saffran et al studies (1996, 1999), backwards and forwards TPs are identical, thus equating the relative predictability of either A or C.}  
Evidence for asymmetries in encoding across syllable positions has been echoed elsewhere [@conway2005modality; @cunillera2010visual]. While Endress and Mehler have hypothesized that SL consists of purely TP-tracking, as opposed to an extraction/chunking-style mechanism, their results also leave some room for other possibilities [-@endress2009surprising]. In their study, learners were familiarized to a language structured such that trisyllabic non-word foils could be created that had high adjacent-syllable TPs, but which had never actually been encountered in the speech stream. For instance, syllable A occurred with syllable B frequently, and syllable B occurred with syllable C frequently, but A had never occurred in a trisyllabic sequence with C. At test, participants chose these non-occurring but high TP sequences as frequently as trisyllabic sequences they had actually encountered in the stream. This was true even when participants were exposed for eight times the original exposure duration: participants still failed to distinguish between the two types of items. The only conditions that led to discrimination between encountered and unencountered high-TP items were those in which prosodic cues signaled the stimuli edges (i.e., small pauses between words, or lengthening of the final syllable vowel durations within the trisyllabic words). On the one hand it appears that participants successfully discovered the underlying bigram TPs, but failed to encode any non-adjacent TP information (Endress and Mehler argue that the non-word foils should have been rejected if participants are encoding the entire trisyllabic sequence). On the other hand, however, the syllables in the high TP foils, though they had never occurred as a unified chunk in the familiarization stream, obeyed positional constraints. Thus, it is possible that participants were relying on positional knowledge, rather than whole string knowledge, and therefore could not reliably distinguish between the two types.  

###Perceptual influences on the SL process.
In contrast to the results reported by Endress and Mehler, a later study that included a replication of that experiment found that learners successfully rejected both part-words and un-encountered but high-TP items [@perruchet2012beyond]. The authors suggested that the discrepancy in results between the two studies may have resulted from a low-level difference in perception introduced by a seemingly small methodological change. The stimuli in both studies were constructed using French sounds; however, the participants in the original study were Italian speakers, unlike those in the replication, who were French speakers. Perruchet and Poulin-Charronat suggested that the Italian-speaking learners in Endress and Mehler [-@endress2009surprising] may have failed to adequately perceive the unfamiliar French speech sounds, whereas their own participants easily encoded their native sounds, and that an inability to accurately encode the encountered sounds impeded the learning mechanism itself. This is consistent with other SL results showing that a lack of familiarity with the stimuli alters learning [@gebhart2009statistical; @graf2015flexibility]. It also conforms with findings from word learning more broadly. For instance, Morrison and Hudson Kam [-@morrison_hudson_kam_2018] showed that phonetic unfamiliarity leads to weaker word-level representations and can impede word learning. And in the infant literature, it is well known that infants in the early stages of word learning will sometimes act as though they have failed to encode phonetic contrasts that they already have representations for, presumably as a result of the word-learning process itself [@stager1997infants; @werker2002infants].    

###Current proposal.  
In sum, SL appears to yield representations that can be built upon and transformed into independent chunks. However, it is less clear what these representations look like before transformation via association with additional cues. On the one hand, learners appear to be sensitive to a range of varying TPs [@goyet2013early; @bogaerts2016splitting], and fail to postulate boundaries that would perfectly segment streams composed of very simple TP structures [@siegelman2017towards]. On the other hand, in some circumstances at least, learners’ representations also appear to involve perceptual grouping of chunks that are defined by high statistical coherence [@fiserEncodingMultielementScenes2005; @zhao2016statistical], or to differ internally in ways that are not easily explained by differences in TPs (Saffran et al., 1999). Previous work also suggests that the familiarity of the stimulus affects learning.  
In the studies that follow, we pit two simplistic accounts of the statistical learning mechanism against each other. We propose that if SL is a process of only tracking TPs between syllables, output representations should reflect only that TP-structure (TP-encoding). Alternatively, if output representations reflect a different kind of property, specifically, encoding of the position of syllables with respect to word boundaries, this is evidence for a different/secondary mechanism that yields independent, word-like chunks (Position-encoding). We test for evidence of these two different learning outcomes by exposing learners to a simple TP-defined familiarization stream, and then comparing learners’ knowledge of syllable positions against their knowledge of the TP structures that they were exposed to (Experiment 1). We also explore the effects of stimulus familiarity on learning. We ask whether these familiarity effects are limited to impeding learning overall, or rather, affect the representations formed during learning. Specifically, we propose that chunk-like behaviour in SL is moderated by the perceptual accessibility of the input stream, and test for enhanced positional effects when the perceptual stream is less familiar (Experiments 2a and 2b).
